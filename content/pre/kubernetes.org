#+TITLE: Building the containers with docker
#+DESCRIPTION: Usa docker para construir contenedores de imágenes para empacar
una aplicación y sus dependencias y así hacer un despliegue en una sola máquina
#+AUTHOR: Sergio Benítez
#+DATE:<2020-11-10 Tue> 

* Profundizando en arquitectura
  
Hasta ahora solo se ha demostrado el trabajo de docker en una única máquina.
Para escalar la infraestructura es necerio pensar en crecimiento externo. 

Los contenedores están jugando un rol importante para el desarrollo de
aplicaciones y el despliegue de las mismas. En el historial de grandes sistemas
en centros de datos se han usado máquinas de metal puro que luego de ser
compradas suelen tener tres años de vida útil para posteriormente depreciarse.
Este ciclo se viene repitiendo una y otra vez. Estás máquinas viejas pueden ser
pensadas como mascotas, pero lo que realmente se quiere es tener un ganado. Bajo
este contexto el hecho de perder una vaca, no significa que se vaya a perder la
producción de leche. Eventualmente se puede conseguir otra vaca. Este es el tipo
de modelo al que las infraestructuras se están proyectando, en vez de tener una
máquinas con características especificas que al ser afectada pueda causar un 
falló total de la aplicación.

La transición de máquinas específicas a rebaños de máquinas, es algo que puede
realizarse con máquinas virtuales, pero dichas máquinas no están diseñadas para
ser usadas en periodos cortos de tiempo. Adicionalmente, eliminar una maquina
virtual es un proceso dispendioso. Por otro lado, los contenedores son rápidos
de crear y destruir dando como resultado un montaje eficiente de infraestructura
. Cabe resaltar que la informática tiende hacia tiempos de inicio y parada aún
más rápidos. Si se lleva este concepto al límite, es posible crear un contenedor
con una simple petición y apagarlo con otra. Esta versatilidad esta abriendo
cabidad al concepto de /serverless computing/ y ya hay algunos frameworks que 
promueven el uso de este concepto.

Ahora, evidentemente hay varios retos a sobrellevar con la escritura de
aplicaciones contenerizadas. El primer inconveniente es que las orgranizaciones
mantienen sus viejas prácticas y por lo tanto sus monólitos. Ya hay una cultura
organizacional establecida al rededor del monolito, y la transición al uso de
micro servicios es un proceso gradual que debe cultivarse en el trabajo interno
de los equipos de desarrollo. Con este nuevo enfoque, se va a tener personas de
cualquier parte del mundo generando aportes al desarrollo de software y eso 
requiere procesos de validación y auditoria al momento de revisar que cambios de
código van a publicarse en la aplicación. Eso implica que la creación de 
software va ser una especie de formulario. Este formulario es a veces llamado la
ley de Conway, la cual dice que las organizaciones diseñan sistemas que imitan 
sus propias estructuras de comunicación. En otras palabras, una empresa no puede
producir microservicos si es una organización con producción en cascada. Por lo
tanto, se pueden perder los benecficios del patrón de microservicos por esquemas
de la organización. 

El segundo inconveniente con la implementación aplicaciones en contenedores son
los obstáculos técnicos. Este patrón requiere de procesos automatizados bien 
ejecutados, El movimiento de piezas hace que los sistemas estén en capacidad de
hacer descubrimientos dinámicos que luego se deben monitoriar. Una pregunta 
válida es si los desarrolladores deberián escribir estos sistemas de 
automatización por cuenta propia, y la respuesta es no. Estas implementaciones
requieren de mucho trabajo y por ende es mejor encontrar un sistema que funcione
y deje la puerta abierta al cambio para infraestructuras venideras, como 
Kubernetes. 

* Como me enteré the Kubernetes (a.k.a k8)

Uno de los retos a los cuales muchas personas se someten en el mercado de la
informática, es escoger entre una gran variedad de teconologías para hacer
tareas puntuales. Está búsequeda puede llegar a ser abrumadora, ya que también 
determina que aprendizajes se deben ejecutar. Para atender esta elección, es
conveniente guiarse de alguién con más experiencia y respaldar la decisión con 
el aporte y el estado de la comunidad detrás de la tecnología. 
  
Para el manejo de aplicaciones una de las mejores herramientas es Kubernetes.
Kubernetes es una herramienta que observa a los contenedores en un nivel
superior. La abstracción sobre la que opera Kubernetes puede llegar a tener
mucho sentido.

* Qué es Kubernetes

Docker hace que el despliegue y la ejecución de los contenedores sea sencillo.
El próximo paso lógico es usar docker en todas las máquinas y buscar una forma
para integrarlas. El empaquetamiento de contenedores corresponde al 5% del
problema. Los inconvenientes reales vienen cuando se trabaja las configuraciones
de las aplicaciones, los descubrimientos de servicios, la administración de 
actualizacione y el monitoreo de las máquinas.

Todas estas carácteristicas se construyen sobre docker, y la recomendación es
tomar ventaja de una plataforma como Kubernetes que se encargue de manejar toda 
esta complejidad de infraestructura.

Kubernetes provee un nuevo conjunto de abstracciones que van más allá de las 
bases del despliegue de contenedores, y habilita a que el desarrollador se
enfoque en el panorama general.

Anteriormente el punto a tratar era el despliegue de aplicaciones en máquinas 
individuales, lo cual funciona en flujos de trabajo limitados. Kubernetes
permite abstraer las máquinas individuales de una forma que el grupo de máquinas
se trate como una única máquina ĺógica. La aplicación se convierte en un
ciudadano de primera clase que habilita su administración a través del uso de
abstracciones de alto nivel.

En Kubernetes se puede describir un conjunto de aplicaciones y como deerian
interactuar entre ellas. Así es como Kubernetes resuelve toda esta complejidad.

* Configurando Kubernetes
  
Use el siguiente directorio de proyecto:

#+begin_src bash
cd $GOPATH/src/github.com/udacity/ud615/kubernetes
#+end_src

o si está en el repositorio del curso:

#+begin_src bash
cd kubernetes
#+end_src

> *Nota:* En cualquier momento puede limipiar la aplicación ejeuctando el script
~cleanup.sh~

** Aprovisiona un clúster de Kubernetes con GKE mediante gcloud

Kubernetes se puede configurar con muchas opciones y complementos, pero puede
llevar mucho tiempo arrancar desde cero. En esta sección, iniciará Kubernetes
con Google Container Engine (GKE).

GKE es un Kubernetes alojado por Google. Los clústeres de GKE se pueden
personalizar y admiten diferentes tipos de máquinas, cantidad de nodos y
configuraciones de red.

Utilice el siguiente comando para crear su clúster y utilizarlo durante el resto
de esta sección.


#+begin_src bash
gcloud container clusters create k0 --zone us-central1-a
#+end_src

* Pequeña demostración de Kubernetes
  
La forma más sencilla de comenzar con Kubernetes es usando el comando ~kubectl~.
Para correr una única instancia del contenedor de nginx se ejecuta el siguiente
comando:

#+begin_src bash
kubectl run nginx --image=nginx:1.10.0
pod/nginx created
#+end_src

Como se puede observar, el mensaje de salida indica que el pod nginx ha sido
creado. Más adelante se desarrollará el concepto de pod, pero por ahora es
suficiente saber que los pods son los contenedores desplegables en kubernetes.

Para revisar el contenedor de nginx que se acaba de crear se esta ejecutando, se
usa el siguiente comando. 

#+begin_src bash
kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          20h
#+end_src

Una vez se sepa que el contenedor de NGINX se esta ejecutando, se puede exponer
por fuera del contexto de Kubernetes con el siguiente comando:

#+begin_src bash
kubectl expose deployment nginx --port 80 --type LoadBalancer
kubectl expose deployment nginx --port 80 --type NodePort
service "nginx" exposed
#+end_src

Detrás de escenas, kubernetes ha creado un balanceador de carga externo con un 
dirección IP pública adjunta. Cualquier cliente que golpee esta dirección IP
pública, será enrutado a los pods que soportan el servicio. Pare este caso
puntual, se tiene el pod de nginx.

Para enlistar los servicios que se están ejecutando, se usa el siguiente comando:

#+begin_src bash
kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.3.240.1   <none>        443/TCP   20h
nginx        ClusterIP   10.3.240.40                443/TCP   31h
#+end_src

En la salida se observa la IP pública que se puede utilizar para golpear el
contenedor nginx remotamente.

Kubernetes soporta una flujo de trabajo sencillo fuera del la caja usando los
comandos ~run~ y ~expose~ de ~kubectl~. Con este breve tour por Kubernetes, es
tiempo de profundizar en cada uno de los componentes y las abstracciones. 

** Hoja de trucos para kubernetes

Este primera aproximación a kubernetes puede ser algo abrumadora. No hay por que
tener, con el tiempo va a ver mayor familiarización con la plataforma. Una ayuda
efectiva es la [[https://kubernetes.io/docs/reference/kubectl/cheatsheet/][hoja de trucos]] de comandos de kubernetes.

* Introducción a los pods

El núcleo de Kubernetes son los pods. Los pods representan una aplicación lógica.
Los pods mantienen una colección de uno o más contenedores. Generalmente, cuando
se tienen múltiples contenedores con una fuerte dependencia entre ellos, se
deben empaquetar dentro de un único pod.

En el ejemplo de referencia, se usa un pod que tiene dos contenedores; uno del
monolito y otro de nginx.

Los pods tambien tienen volúmenes. los volúmenes son divisiones de datos que
viven tanto tiempo como el pod y pueden ser utilizados por cualquiera de los
contenedores ese pod. Esto es posible porque los pods proporcionan un espacio de
nombres compartido para su contenido.

Eso significa que los dos contenedores dentro de pod de ejemplo se pueden
comunicar entre ellos, y compartir los volumenes adjuntos.

Los pods también comparten un espacio de nombre de red, lo que implica que hay
una dirección IP por pod.

La siguiente ilustración sirve para aglomerar todos los conceptos que componen
un pod:

#+CAPTION: El cliente envía una solicitud de log in
[[../images/microservices/01-kubernetes-pod.png]]

* Creando pods

Los pods son creados a través de archivos de configuración pod. A continuación
se revisará el modelo del archivo de configuración del pod que se está usando
como ejemplo:

#+begin_src bash
$ cat pods/monolith.yalm

apiVersion: v1
kind: Pod
metadata:
  name: monolith
  labels:
    app: monolith
spec:
  containers:
    - name: monolith
      image: udacity/example-monolith:1.0.0
      args:
        - "-http=0.0.0.0:80"
        - "-health=0.0.0.0:81"
        - "-secret=secret"
      ports:
        - name: http
          containerPort: 80
        - name: health
          containerPort: 81
      resources:
        limits:
          cpu: 0.2
          memory: "10Mi"
#+end_src

Se tiene información relevante en este archivo, como por ejemplo, que el pod
solo tiene un contenedor llamado monolith. También se puede observar que se
están pasando tres argumentos al contenedor al momento de iniciarse y por último
, El puerto 80 está abierto para tráfico HTTP, y el puerto 81 para health checks.

Para crear el pod del monolith, se usa ~kubectl~ con el siguiente comando:

#+begin_src bash
$ kubectl create -f pods/monolith.yaml
pod "monolith" created
#+end_src

Para examinar los pod, se ejecuta el siguiente comando para obtener una lista de
todos los pods que se están corriendo en el espacio de nombre por defecto:

#+begin_src bash
$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
monolith   1/1     Running   0          11s
nginx      1/1     Running   0          8d
#+end_src

Se tomará algunos segundos mientras que el pod monolith este arriba y corriendo,
ya que el contenedor de la imagen del monolíto necesita ser traído desde el 
docker hub antes de poder correrlo.

Con el comando de descripción, se obtiene varia información sobre el pod
monolito, desde la dirección IP hasta el log de eventos. Esta información
resulta práctica para la solución de problemas.

#+begin_src bash
$ kubectl describe pods monolith
#+end_src

Como se puede observar, Kubernetes hace fácil la creación de pods a través de un
arhcivo de configuración y la visualización de los mismo cuando se están
corriendo. En este punto se tiene la habilidad de crear todos los pods que de 
acuerdo a los requisitos de despliegue.

* Interactuando con los pods
* Revisión general de MHC
* Configuración de la aplicación
* Revisión general sobre seguridad
* Creando secretos
* Accediendo a una endpoint HTTPS
* Revisión general de servicios
* Creando servicios
* Agregando etiquetas a los pods
* Outro



  
