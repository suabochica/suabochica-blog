#+TITLE: Project
#+DESCRIPTION: Uso de Kubernetes en aplicaciones reales
#+AUTHOR: Sergio Benítez
#+DATE:<2020-12-31 Thu> 

* Intro

Es tiempo de ir más lejos con el apredizaje de Kubernetes. Ya se repasaron los
conceptos, y solo falta revisar como gerenciar Kubernetes con aplicativos reales.

Generalmente las aplicaciones reales exigen almacenamiento persistente,
búsquedas TLS e implementaciones más avanzadas. Hasta el momento, Kubernetes ha
parecido una mejor forma de ejecutar comandos Docker. Por ejemplo, con los
comandos de ~kubectl~ se logra desplegar contenedores, y hacerlos disponibles
por fuera del cluster.

En casos avanzados, es ideal declarar el estado desado de las infraestructuras y
eso implica correr configuraciones en Kubernetes. Las configuraciones en k8s,
permiten administrar los secretos, y determinar el flujo para actualizar los
aplicativos.

Ahora, ¿qué siginifica el estado deseado?. Practicamente sería crear un conjunto
de instrucciones hacia Kubernetes, y tras bastidores, la herramienta hará todo
el trabajo, como la implementación de contenedores y la creación de
balanceadores de carga.

Se prosigue con el uso de k8s en casos más avanzados.

* Revisión general de un despliegue

El objetivo con Docker y Kubernetes es estar en capacidad de escalar y manejar
contenedores en producción, y es aquí donde los despliegues empiezan a tener el
protragonismo.

Los despliegues son formas declarativas para decir que va en donde. El uso de su
estados es dado por el usuario final. Destrás de escenas, los displiegues en k8s
usan un concepto llamado conjuntos de replica con el fin de asegurar que el
número actual de pods es igual al número deseado.

Al momento de administrar los pods, el despliegue debe ser consciente de
detalles de bajo nivel, como en que nodo esta el pod y en donde se esta
ejecutando. El tiempo de vida del pod esta ligado al del nodo, lo que implica
que si el nodo se cae, el por también lo hará.

Está administración de nodos se le delega e los despliegues, ya que con una
serie de instrucciones se puede gestar efectivamente los estados de los nodos.

En el ejemplo de la siguiente imagen, se muestra un despliegue con tres replicas.
Una de ellas está caida y la misma configuración estará en capacidad de iniciar
un nuevo pod y encontrar un lugar para él. En este caso, el pod se inicia en el
segundo nodo, comportamiento que resulta bastante útil.

#+CAPTION: El cliente envía una solicitud de log in
[[../images/microservices/01-project-deployments.png]]

Es tiempo de combinar todo lo que se ha aprendido hasta ahora de pods y servicios
y romper un aplicación monolito en pequeños servicios usando despliegues.

* Creando despliegues

Es momento de crear despliegues, uno por cada servicio: ~auth~, ~frontend~ y
~hello~. Se van a definir servicios internos para los despliegues de ~auth~ y
~hello~ y un servicio externo para el despliegue del ~frontend~.

Es recomendable empezar con revisar los contenidos de los archivos de
configuración. Para este caso puntual vamos a revisar el archivo ~auth.yalm~

#+begin_src bash
$ cat deployments/auth.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "auth"
  template:
    metadata:
      labels:
        app: auth
        track: stable
    spec:
      containers:
        - name: auth
          image: "udacity/example-auth:1.0.0"
          ports:
            - name: http
              containerPort: 80
            - name: health
              containerPort: 81
          resources:
            limits:
              cpu: 0.2
              memory: "10Mi"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 81
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /readiness
              port: 81
              scheme: HTTP
            initialDelaySeconds: 5
            timeoutSeconds: 1
#+end_src

En esta salida se obtiene información útil como el número de replicas, las
etiquetas que se van a usar y la versión de la imagen de docker sobre la cual se
va a generar el contenedor. Se resalta que dicha configurarción se puede editar
cuando se requiera necesario.

Para crear el despliegue del servicio se usa el siguiente comando:

#+begin_src bash
$ kubectl create -f deployments/auth.yaml
deployment.apps/auth created
#+end_src

Al igual que cualquier otro objeto kubernetes, se puede usar el comando
~describe~ para obtener más información sobre el despliegue del servicio:

#+begin_src bash
$ kubectl describe deployments auth
Name:                   auth
Namespace:              default
CreationTimestamp:      Mon, 04 Jan 2021 21:02:35 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=auth
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=auth
           track=stable
  Containers:
   auth:
    Image:       udacity/example-auth:1.0.0
    Ports:       80/TCP, 81/TCP
    Host Ports:  0/TCP, 0/TCP
    Limits:
      cpu:        200m
      memory:     10Mi
    Liveness:     http-get http://:81/healthz delay=5s timeout=5s period=15s #success=1 #failure=3
    Readiness:    http-get http://:81/readiness delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   auth-784c79df7f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  99s   deployment-controller  Scaled up replica set auth-784c79df7f to 1
#+end_src

Con esta configurarción, ya se está en capacidad de crear el servicio de ~auth~,
siguiendo un proceso similar al que se descrbió anteriormente:

#+begin_src bash
$ kubectl create -f services/auth.yaml
services.apps/auth created
#+end_src

Se repiten estos pasos con cada uno de los servicios:

#+begin_src bash
$ kubectl create -f deployments/hello.yaml
deployment.apps/hello created

$ kubectl create -f services/hello.yaml
services.apps/hello created

$ kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf
configmap/nginx-frontend-conf created

$ kubectl create -f deployments/frontend.yaml
deployment.apps/frontend created

$ kubectl create -f services/frontend.yaml
services.apps/frontend created
#+end_src

Para el caso del servicio frontend, es necesario configurar NGINX tal y como se
reviso previamente, a través del ~configmap.

Con dicha configuración, se esta listo para interectuar con el servicio
~frontend~ agarrando la dirección IP externa y usando ~curl~ para golpearla.

#+begin_src bash
$ kubectl get services frontend
NAME       TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)         AGE
frontend   LoadBalancer   10.3.241.59   35.188.198.204   443:31017/TCP   5m47s

$ curl -k https://35.188.198.204
{"message": "hello"}
#+end_src

Y esto es todo, ahora se tiene una aplicación de multiservicios desplegada
usando k8s. Estas habilidades permitirán desplegar applicaciones más complicadas
sobre k8s, usando colecciones de despliegues y servicios.

* Revisión generar de escalamiento

El escalamiento es hecho con la actualización del valor en la propuedad replicas
de nuestro manifiesto de despliegue. Esta es considerada una buena práctica, ya
que a pesar de tener métodos imperativos con ~kubectl scale~ no hay estado
guardado en ninguna parte.

Por debajo, los despliegues crean un conjunto de replicas para manejar la
creación, supresión y actulización de pods. Los despliegues administran por
cuenta propia los conjuntos de replicas y por ende no se debe preocuparse por
ellos.

Esta característica de los despliegues hace que el escalamiento hacia arriba y
hacia abajo sea igual de facil para uno, dos o n nodos. En la siguiente imagen
se muestra como el despliegue de ~auth~ se amplia por tres replicas:

#+CAPTION: El cliente envía una solicitud de log in
[[../images/microservices/02-project-scaling.png]]

* Despliegues de escalamiento

Es importante tener presente que cada despliegue es mapeado hacia un conjunto de
replicas activo. El siguiente comando permite visualizar el conjunto actual de
replicas que se están ejecutando:

#+begin_src bash
$ kubectl get replicasets
NAME                DESIRED   CURRENT   READY   AGE
auth-784c79df7f     1         1         1       3h38m
frontend-868c46fc   1         1         0       3h31m
hello-67dfcd5745    1         1         1       3h32m
nginx-26dfcd5745    1         1         1       3h32m
#+end_src

Los conjuntos de replicas son escalados hacia los despliegues por cada servicio
y este proceso se puede hacer de manera independiente. Como se menciono
anteriormente, la verdadera fortaleza de k8s viene cuando se trabaja de manera
declarativa, en vez de usar comandos imperativos de kubctl.

Si se quiere ver cuantos pods de ~hello~ se estan corriendo, se ejecuta el
siguiente comando:

#+begin_src bash
$ kubectl get pods -l "app=hello,track=stable"
NAME
hello-67dfcd5745
#+end_src

Para escalar el despliegue ~hello~ con tres replicas, se debe actualizar la
propiedad ~replicas: 3~ en el archivo ~deployments/hello.yaml~. Para aplicar los
cambios se corre el siguiente comando:

#+begin_src bash
$ kubectl apply -f deployments/hello.yaml
deployment "hello" configured
#+end_src

Se ejecuta nuevamente el ~replicasets~ y se verán los cambios:

#+begin_src bash
$ kubectl get replicasets
NAME                DESIRED   CURRENT   READY   AGE
auth-784c79df7f     1         1         1       3h38m
frontend-868c46fc   1         1         0       3h31m
hello-67dfcd5745    3         3         3       3h32m
nginx-26dfcd5745    1         1         1       3h32m
#+end_src

El valor ~DESIRED~ para indicar el número deseado de replicas fue actualizado.
Al correr el comando ~kubectl get pods~ también se observa el cambio:

#+begin_src bash
$ kubectl get pods
NAME                      READY   STATUS              RESTARTS   AGE
auth-784c79df7f-jwdgp     1/1     Running             0          3h52m
frontend-868c46fc-k7nl5   0/1     ContainerCreating   0          3h45m
hello-67dfcd5745-7fswt    1/1     Running             0          3h45m
hello-67dfcd5745-7fsst    1/1     Running             0          3h45m
hello-67dfcd5745-7fszt    1/1     Running             0          3h45m
nginx-87dfcd5745-7fszt    1/1     Running             0          3h45m
monolith                  1/1     Running             0          3h45m
secure-monolith           1/1     Running             0          3h45m
#+end_src

Similarmente, el comando describe sobre ~hello~ es consistente con el resultado:

#+begin_src bash
$ kubectl describe deployment hello
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=hello
            track=stable
  Containers:
    hello:
    Image:       udacity/example-hello:1.0.0
    Ports:       80/TCP, 81/TCP
    Host Ports:  0/TCP, 0/TCP
    Limits:
      cpu:        200m
      memory:     10Mi
    Liveness:     http-get http://:81/healthz delay=5s timeout=5s period=15s #success=1 #failure=3
    Readiness:    http-get http://:81/readiness delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   hello-67dfcd5745 (3/3 replicas created)
Events:          <none>
#+end_src

En este punto se tienen múltiples copias del servicio ~hello~ corriendo en k8s y
se tiene un solo servicio frontend que esta de intermediario sobre el tráfico
hacia los tres pods. Esto perimite compartir la carga y escalar los contenedores
en k8s.

* Descripción general de actualizaciones

* Actualizaciones continuas

* Outro
